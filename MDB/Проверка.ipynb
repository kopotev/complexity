{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPNIQ/iVVJeCuiLdXK62OYF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Vakhranev/MDB/blob/main/%D0%9F%D1%80%D0%BE%D0%B2%D0%B5%D1%80%D0%BA%D0%B0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install lexical-diversity"
      ],
      "metadata": {
        "id": "FhSNQKRoC8Qm",
        "outputId": "e3c4b6ad-a161-4f8f-b594-8a2f598eb476",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting lexical-diversity\n",
            "  Downloading lexical_diversity-0.1.1-py3-none-any.whl (117 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.8/117.8 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: lexical-diversity\n",
            "Successfully installed lexical-diversity-0.1.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import math\n",
        "import re\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "from nltk import word_tokenize\n",
        "from collections import Counter\n",
        "from math import comb\n",
        "from lexical_diversity import lex_div as ld\n",
        "\n",
        "class MetricsCalculator:\n",
        "    def __init__(self, threshold=0.8):\n",
        "        self.threshold = threshold\n",
        "        self.ttr_data = {}\n",
        "        self.vocd_data = {}\n",
        "        self.hd_data = {}\n",
        "        self.mtld_data = {}\n",
        "\n",
        "    def preprocess_text(self, text):\n",
        "        text = re.sub(r'<.*?>', '', text)\n",
        "        text = re.sub(r'\\n', '', text)\n",
        "        return text\n",
        "\n",
        "    def calculate_ttr(self, lemmas):\n",
        "        total_lemmas = len(lemmas)\n",
        "        unique_lemmas = len(set(lemmas))\n",
        "        ttr = unique_lemmas / total_lemmas\n",
        "        return ttr\n",
        "\n",
        "    def calculate_vocd_d(self, lemmas):\n",
        "        lemma_count = len(lemmas)\n",
        "        unique_lemmas = set(lemmas)\n",
        "        unique_lemma_count = len(unique_lemmas)\n",
        "\n",
        "        freq_dict = Counter(lemmas)\n",
        "        freq_values = list(freq_dict.values())\n",
        "\n",
        "        freq_sum = sum(freq_values)\n",
        "        sorted_freqs = sorted(freq_values, reverse=True)\n",
        "\n",
        "        cumulative_freq = 0\n",
        "        vocd_d = None\n",
        "\n",
        "        for idx, freq in enumerate(sorted_freqs):\n",
        "            cumulative_freq += freq\n",
        "            if cumulative_freq / freq_sum >= self.threshold:\n",
        "                vocd_d = (idx + 1) / unique_lemma_count\n",
        "                break\n",
        "\n",
        "        return vocd_d\n",
        "\n",
        "    def process_files(self, file_paths):\n",
        "        vocd_values = []\n",
        "        hd_values = []\n",
        "        mtld_values = []\n",
        "\n",
        "        for filepath in file_paths:\n",
        "            with open(filepath, \"r\", encoding=\"utf-8\") as file:\n",
        "                text = file.read()\n",
        "                preprocessed_text = self.preprocess_text(text)\n",
        "                tokens = word_tokenize(preprocessed_text)\n",
        "                lemmatizer = nltk.WordNetLemmatizer()\n",
        "                lemmas = [lemmatizer.lemmatize(token.lower()) for token in tokens if token.isalpha()]\n",
        "                vocd = self.calculate_vocd_d(lemmas)\n",
        "                hd = ld.hdd(lemmas)\n",
        "                mtld = ld.mtld(lemmas)\n",
        "                vocd_values.append(vocd)\n",
        "                hd_values.append(hd)\n",
        "                mtld_values.append(mtld)\n",
        "\n",
        "        return vocd_values, hd_values, mtld_values\n",
        "\n",
        "    def calculate_avg_metrics(self, vocd_values, hd_values, mtld_values):\n",
        "        avg_vocd = sum(vocd_values) / len(vocd_values)\n",
        "        avg_hd = sum(hd_values) / len(hd_values)\n",
        "        avg_mtld = sum(mtld_values) / len(mtld_values)\n",
        "\n",
        "        return avg_vocd, avg_hd, avg_mtld\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Задайте пути к вашим файлам 1.txt, 2.txt и 3.txt\n",
        "    file_paths = [\"1.txt\", \"2.txt\", \"3.txt\"]\n",
        "\n",
        "    metrics_calculator = MetricsCalculator(threshold=0.72)\n",
        "    vocd_values, hd_values, mtld_values = metrics_calculator.process_files(file_paths)\n",
        "\n",
        "    for i, (vocd, hd, mtld) in enumerate(zip(vocd_values, hd_values, mtld_values)):\n",
        "        print(f\"File {i + 1} ({file_paths[i]}):\")\n",
        "        print(f\"  voc-D: {vocd:.4f}\")\n",
        "        print(f\"  HD-D: {hd:.4f}\")\n",
        "        print(f\"  MTLD: {mtld}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GTLPhdCB-RoR",
        "outputId": "2d5c0a01-95b5-42a2-cfed-8e6a8938a350"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File 1 (1.txt):\n",
            "  voc-D: 1.0000\n",
            "  HD-D: 0.0000\n",
            "  MTLD: 3.1111111111111116\n",
            "File 2 (2.txt):\n",
            "  voc-D: 0.8000\n",
            "  HD-D: 0.0000\n",
            "  MTLD: 0.0\n",
            "File 3 (3.txt):\n",
            "  voc-D: 0.8000\n",
            "  HD-D: 0.0000\n",
            "  MTLD: 5.6000000000000005\n"
          ]
        }
      ]
    }
  ]
}