{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOLiFtV9/OY57LDnP9+Rc4W",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Vakhranev/MDB/blob/main/Mean%2C%20median%20%D0%B8%20SD.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Средняя длина токена"
      ],
      "metadata": {
        "id": "Cm-xPV9D3PMh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WidsTOkX3HAp",
        "outputId": "6119735b-96d4-4e8c-edee-1a02a29e3b25"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Статистики для длины токенов по уровням CEFR:\n",
            "A1: Mean = 5.113701933523286, Median = 4.887096774193548, SD = 1.3934970447824555\n",
            "A2: Mean = 4.944421601083338, Median = 4.913850194108244, SD = 0.41792530525382815\n",
            "B1: Mean = 5.3628620491586645, Median = 5.213872832369942, SD = 0.7346305939965745\n",
            "B2: Mean = 5.801422150236, Median = 5.594594594594595, SD = 0.8730457684630442\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import glob\n",
        "import numpy as np\n",
        "\n",
        "cefr_levels = [\"A1\", \"A2\", \"B1\", \"B2\"]\n",
        "\n",
        "# Словарь для хранения статистик для каждого уровня CEFR\n",
        "token_length_stats = {}\n",
        "\n",
        "# Обработка каждого уровня CEFR\n",
        "for cefr_level in cefr_levels:\n",
        "    folder_path = f\"/content/Students_texts/{cefr_level}\"\n",
        "    file_list = glob.glob(os.path.join(folder_path, \"*.txt\"))\n",
        "\n",
        "    # Список для хранения средних значений длин токенов в текущем уровне CEFR\n",
        "    lengths_in_level = []\n",
        "\n",
        "    # Обработка каждого файла в текущем уровне CEFR\n",
        "    for file_path in file_list:\n",
        "        with open(file_path, 'r') as file:\n",
        "            text = file.read()\n",
        "            tokens = text.split()\n",
        "            token_lengths = [len(token) for token in tokens]\n",
        "            avg_token_length = np.mean(token_lengths)\n",
        "            lengths_in_level.append(avg_token_length)\n",
        "\n",
        "    # Вычисление средних значений, медианы и стандартного отклонения для текущего уровня CEFR\n",
        "    mean_length = np.mean(lengths_in_level)\n",
        "    median_length = np.median(lengths_in_level)\n",
        "    sd_length = np.std(lengths_in_level)\n",
        "\n",
        "    # Сохранение статистик в словарь\n",
        "    token_length_stats[cefr_level] = {\n",
        "        'mean': mean_length,\n",
        "        'median': median_length,\n",
        "        'sd': sd_length\n",
        "    }\n",
        "\n",
        "# Вывод результатов\n",
        "print(\"Статистики для длины токенов по уровням CEFR:\")\n",
        "for cefr_level in cefr_levels:\n",
        "    stats = token_length_stats[cefr_level]\n",
        "    print(f\"{cefr_level}: Mean = {stats['mean']}, Median = {stats['median']}, SD = {stats['sd']}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pymorphy2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tmdApxWUEyeu",
        "outputId": "a52263f7-e00d-459d-d14b-d5d3b889c440"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pymorphy2\n",
            "  Downloading pymorphy2-0.9.1-py3-none-any.whl.metadata (3.6 kB)\n",
            "Collecting dawg-python>=0.7.1 (from pymorphy2)\n",
            "  Downloading DAWG_Python-0.7.2-py2.py3-none-any.whl.metadata (7.0 kB)\n",
            "Collecting pymorphy2-dicts-ru<3.0,>=2.4 (from pymorphy2)\n",
            "  Downloading pymorphy2_dicts_ru-2.4.417127.4579844-py2.py3-none-any.whl.metadata (2.1 kB)\n",
            "Collecting docopt>=0.6 (from pymorphy2)\n",
            "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Downloading pymorphy2-0.9.1-py3-none-any.whl (55 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.5/55.5 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading DAWG_Python-0.7.2-py2.py3-none-any.whl (11 kB)\n",
            "Downloading pymorphy2_dicts_ru-2.4.417127.4579844-py2.py3-none-any.whl (8.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.2/8.2 MB\u001b[0m \u001b[31m76.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: docopt\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13704 sha256=837ae5f2e62452b9e95a462ef63cd55914e9f546e9f50a6759ba934f669f3d16\n",
            "  Stored in directory: /root/.cache/pip/wheels/fc/ab/d4/5da2067ac95b36618c629a5f93f809425700506f72c9732fac\n",
            "Successfully built docopt\n",
            "Installing collected packages: pymorphy2-dicts-ru, docopt, dawg-python, pymorphy2\n",
            "Successfully installed dawg-python-0.7.2 docopt-0.6.2 pymorphy2-0.9.1 pymorphy2-dicts-ru-2.4.417127.4579844\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Среднее количество слогов"
      ],
      "metadata": {
        "id": "BfhBw1gJJ2cm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import glob\n",
        "import numpy as np\n",
        "import pymorphy2\n",
        "\n",
        "# Создаем экземпляр класса MorphAnalyzer для работы с морфологией русского языка\n",
        "morph = pymorphy2.MorphAnalyzer()\n",
        "\n",
        "# Уровни CEFR\n",
        "cefr_levels = [\"A1\", \"A2\", \"B1\", \"B2\"]\n",
        "\n",
        "# Словарь для хранения статистик для каждого уровня CEFR\n",
        "syllable_stats = {}\n",
        "\n",
        "# Обработка каждого уровня CEFR\n",
        "for cefr_level in cefr_levels:\n",
        "    folder_path = f\"/content/Students_texts/{cefr_level}\"\n",
        "    file_list = glob.glob(os.path.join(folder_path, \"*.txt\"))\n",
        "\n",
        "    # Список для хранения средних значений количества гласных в токене в текущем уровне CEFR\n",
        "    avg_vowel_counts = []\n",
        "\n",
        "    # Обработка каждого файла в текущем уровне CEFR\n",
        "    for file_path in file_list:\n",
        "        with open(file_path, 'r') as file:\n",
        "            text = file.read()\n",
        "            tokens = text.split()\n",
        "\n",
        "            total_vowel_count = 0\n",
        "            token_count = 0\n",
        "\n",
        "            # Вычисляем количество гласных букв для каждого токена\n",
        "            for token in tokens:\n",
        "                parsed_token = morph.parse(token)[0]\n",
        "\n",
        "                # Проверяем, есть ли нормализованная форма слова\n",
        "                if parsed_token.normal_form is not None:\n",
        "                    normalized_word = parsed_token.normal_form\n",
        "                    vowel_count = sum(letter in 'аеёиоуыэюя' for letter in normalized_word.lower())\n",
        "                    total_vowel_count += vowel_count\n",
        "                    token_count += 1\n",
        "\n",
        "            # Среднее количество гласных букв для файла\n",
        "            avg_vowel_count = total_vowel_count / token_count if token_count > 0 else 0\n",
        "            avg_vowel_counts.append(avg_vowel_count)\n",
        "\n",
        "    # Вычисление статистик для текущего уровня CEFR\n",
        "    mean_syllable_length = np.mean(avg_vowel_counts)\n",
        "    median_syllable_length = np.median(avg_vowel_counts)\n",
        "    sd_syllable_length = np.std(avg_vowel_counts)\n",
        "\n",
        "    # Сохранение статистик в словарь\n",
        "    syllable_stats[cefr_level] = {\n",
        "        'mean': mean_syllable_length,\n",
        "        'median': median_syllable_length,\n",
        "        'sd': sd_syllable_length\n",
        "    }\n",
        "\n",
        "# Вывод результатов\n",
        "print(\"Статистики для средней длины слога по уровням CEFR:\")\n",
        "for cefr_level in cefr_levels:\n",
        "    stats = syllable_stats[cefr_level]\n",
        "    print(f\"{cefr_level}: Mean = {stats['mean']}, Median = {stats['median']}, SD = {stats['sd']}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rAKRyaroErTK",
        "outputId": "3b1eea4c-f2e4-40ae-ade8-79f2df6c4901"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Статистики для средней длины слога по уровням CEFR:\n",
            "A1: Mean = 1.7497540186671852, Median = 1.7272727272727273, SD = 0.24646677238109457\n",
            "A2: Mean = 1.8334484841600103, Median = 1.812985751295337, SD = 0.18405719968034648\n",
            "B1: Mean = 1.9400480756646419, Median = 1.9276315789473684, SD = 0.17845087384591238\n",
            "B2: Mean = 2.043987996429243, Median = 2.038327526132404, SD = 0.171317102967923\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Среднее количество морфем"
      ],
      "metadata": {
        "id": "b2mpPgumJ6gE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import glob\n",
        "import re\n",
        "import numpy as np\n",
        "import pymorphy2\n",
        "\n",
        "# Путь к папке с уровнями CEFR\n",
        "cefr_levels = [\"A1\", \"A2\", \"B1\", \"B2\"]\n",
        "\n",
        "# Имя файла со словарем морфем\n",
        "dictionary_file = \"cleaned_tihonov (5).txt\"\n",
        "\n",
        "# Инициализация pymorphy\n",
        "morph = pymorphy2.MorphAnalyzer()\n",
        "\n",
        "# Считываем словарь морфем\n",
        "morpheme_dictionary = {}\n",
        "with open(dictionary_file, 'r', encoding='utf-8') as dict_file:\n",
        "    for line in dict_file:\n",
        "        line = line.strip()\n",
        "        if line:\n",
        "            word, morphemes = line.split()\n",
        "            morpheme_count = morphemes.count('/') + morphemes.count('(') + 1\n",
        "            morpheme_dictionary[word.lower()] = morpheme_count\n",
        "\n",
        "# Словарь для хранения статистик для каждого уровня CEFR\n",
        "morpheme_stats = {}\n",
        "\n",
        "# Обработка каждого уровня CEFR\n",
        "for cefr_level in cefr_levels:\n",
        "    folder_path = f\"/content/Students_texts/{cefr_level}\"\n",
        "    file_list = glob.glob(os.path.join(folder_path, \"*.txt\"))\n",
        "\n",
        "    avg_morpheme_counts = []\n",
        "\n",
        "    # Обработка каждого файла в текущем уровне CEFR\n",
        "    for file_path in file_list:\n",
        "        with open(file_path, 'r', encoding='utf-8') as file:\n",
        "            text = file.read()\n",
        "            tokens = re.findall(r'\\b\\w+\\b', text)\n",
        "\n",
        "            total_morpheme_count = 0\n",
        "            token_count = 0\n",
        "\n",
        "            # Вычисляем количество морфем для каждого токена\n",
        "            for token in tokens:\n",
        "                lemma = morph.parse(token)[0].normal_form\n",
        "                if lemma in morpheme_dictionary:\n",
        "                    morpheme_count = morpheme_dictionary[lemma]\n",
        "                    total_morpheme_count += morpheme_count\n",
        "                    token_count += 1\n",
        "\n",
        "            # Среднее количество морфем для файла\n",
        "            avg_morpheme_count = total_morpheme_count / token_count if token_count > 0 else 0\n",
        "            avg_morpheme_counts.append(avg_morpheme_count)\n",
        "\n",
        "    # Вычисление статистик для текущего уровня CEFR\n",
        "    mean_morpheme_count = np.mean(avg_morpheme_counts)\n",
        "    median_morpheme_count = np.median(avg_morpheme_counts)\n",
        "    sd_morpheme_count = np.std(avg_morpheme_counts)\n",
        "\n",
        "    # Сохранение статистик в словарь\n",
        "    morpheme_stats[cefr_level] = {\n",
        "        'mean': mean_morpheme_count,\n",
        "        'median': median_morpheme_count,\n",
        "        'sd': sd_morpheme_count\n",
        "    }\n",
        "\n",
        "# Вывод результатов\n",
        "print(\"Статистики для среднего количества морфем по уровням CEFR:\")\n",
        "for cefr_level in cefr_levels:\n",
        "    stats = morpheme_stats[cefr_level]\n",
        "    print(f\"{cefr_level}: Mean = {stats['mean']}, Median = {stats['median']}, SD = {stats['sd']}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S3mNqAR9JyDy",
        "outputId": "f3540a91-bcb4-4bed-efab-82291ae2d1d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Статистики для среднего количества морфем по уровням CEFR:\n",
            "A1: Mean = 1.8635769204088146, Median = 1.8461538461538463, SD = 0.21898509867651822\n",
            "A2: Mean = 1.891213743383351, Median = 1.8925452839719998, SD = 0.13275444502008443\n",
            "B1: Mean = 1.972342362735188, Median = 1.95906432748538, SD = 0.13562527054802923\n",
            "B2: Mean = 2.057176504070247, Median = 2.04149377593361, SD = 0.12775786008582243\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Среднее количество уникальных токенов"
      ],
      "metadata": {
        "id": "BSSuraqWMOMe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import glob\n",
        "import re\n",
        "import pymorphy2\n",
        "import numpy as np\n",
        "\n",
        "# Инициализация морфологического анализатора pymorphy2\n",
        "morph_analyzer = pymorphy2.MorphAnalyzer()\n",
        "\n",
        "# Путь к папке с уровнями CEFR\n",
        "cefr_levels = [\"A1\", \"A2\", \"B1\", \"B2\"]\n",
        "\n",
        "# Словарь для хранения статистик для каждого уровня CEFR\n",
        "unique_token_stats = {}\n",
        "\n",
        "# Обработка каждого уровня CEFR\n",
        "for cefr_level in cefr_levels:\n",
        "    folder_path = f\"/content/Students_texts/{cefr_level}\"\n",
        "    file_list = glob.glob(os.path.join(folder_path, \"*.txt\"))\n",
        "\n",
        "    unique_token_counts = []\n",
        "\n",
        "    # Обработка каждого файла в текущем уровне CEFR\n",
        "    for file_path in file_list:\n",
        "        with open(file_path, 'r', encoding='utf-8') as file:\n",
        "            text = file.read()\n",
        "\n",
        "            # Удаление тегов типа <tag> и [unclear]\n",
        "            text = re.sub(r'<[^>]+>', '', text)\n",
        "            text = text.replace('[unclear]', '')\n",
        "\n",
        "            # Разделение текста на слова\n",
        "            words = re.findall(r'\\b\\w+\\b', text)\n",
        "\n",
        "            # Список для хранения уникальных слов\n",
        "            unique_words = set()\n",
        "\n",
        "            # Обработка каждого слова\n",
        "            for word in words:\n",
        "                if not word.strip():\n",
        "                    continue\n",
        "\n",
        "                # Лемматизация слова и получение его грамматических признаков\n",
        "                parsed_word = morph_analyzer.parse(word.lower())[0]\n",
        "                normal_form = parsed_word.normal_form\n",
        "                grammatical_features = parsed_word.tag.__str__()\n",
        "\n",
        "                # Формируем строку из нормальной формы слова и его грамматических признаков\n",
        "                word_info = f\"{normal_form} {grammatical_features}\"\n",
        "\n",
        "                # Добавляем слово в список уникальных слов\n",
        "                unique_words.add(word_info)\n",
        "\n",
        "            # Количество уникальных слов в файле\n",
        "            unique_word_count = len(unique_words)\n",
        "            unique_token_counts.append(unique_word_count)\n",
        "\n",
        "    # Вычисление статистик для текущего уровня CEFR\n",
        "    mean_unique_token_count = np.mean(unique_token_counts)\n",
        "    median_unique_token_count = np.median(unique_token_counts)\n",
        "    sd_unique_token_count = np.std(unique_token_counts)\n",
        "\n",
        "    # Сохранение статистик в словарь\n",
        "    unique_token_stats[cefr_level] = {\n",
        "        'mean': mean_unique_token_count,\n",
        "        'median': median_unique_token_count,\n",
        "        'sd': sd_unique_token_count\n",
        "    }\n",
        "\n",
        "# Вывод результатов\n",
        "print(\"Статистики для среднего количества уникальных токенов по уровням CEFR:\")\n",
        "for cefr_level in cefr_levels:\n",
        "    stats = unique_token_stats[cefr_level]\n",
        "    print(f\"{cefr_level}: Mean = {stats['mean']}, Median = {stats['median']}, SD = {stats['sd']}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-jsLY4kzMR5f",
        "outputId": "f6bfceee-d68e-4771-a112-a5f0e628bd05"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Статистики для среднего количества уникальных токенов по уровням CEFR:\n",
            "A1: Mean = 38.888888888888886, Median = 38.0, SD = 21.337879806542354\n",
            "A2: Mean = 86.3013698630137, Median = 83.0, SD = 43.90234646941705\n",
            "B1: Mean = 123.1044776119403, Median = 118.0, SD = 49.29212950485807\n",
            "B2: Mean = 164.40944881889763, Median = 168.0, SD = 47.54730000211868\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Среднее количество уникальных лемм"
      ],
      "metadata": {
        "id": "DJ9CvL6eOD8a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import glob\n",
        "import re\n",
        "import pymorphy2\n",
        "import numpy as np\n",
        "\n",
        "# Инициализация морфологического анализатора pymorphy2\n",
        "morph_analyzer = pymorphy2.MorphAnalyzer()\n",
        "\n",
        "# Путь к папке с уровнями CEFR\n",
        "cefr_levels = [\"A1\", \"A2\", \"B1\", \"B2\"]\n",
        "\n",
        "# Словарь для хранения статистик для каждого уровня CEFR\n",
        "unique_lemma_stats = {}\n",
        "\n",
        "# Обработка каждого уровня CEFR\n",
        "for cefr_level in cefr_levels:\n",
        "    folder_path = f\"/content/Students_texts/{cefr_level}\"\n",
        "    file_list = glob.glob(os.path.join(folder_path, \"*.txt\"))\n",
        "\n",
        "    unique_lemma_counts = []\n",
        "\n",
        "    # Обработка каждого файла в текущем уровне CEFR\n",
        "    for file_path in file_list:\n",
        "        with open(file_path, 'r', encoding='utf-8') as file:\n",
        "            text = file.read()\n",
        "\n",
        "            # Удаление тегов типа <tag> и [unclear]\n",
        "            text = re.sub(r'<[^>]+>', '', text)\n",
        "            text = text.replace('[unclear]', '')\n",
        "\n",
        "            # Разделение текста на слова\n",
        "            words = re.findall(r'\\b\\w+\\b', text)\n",
        "\n",
        "            # Множество для хранения уникальных лемм с частями речи\n",
        "            unique_lemmas = set()\n",
        "\n",
        "            # Обработка каждого слова\n",
        "            for word in words:\n",
        "                if not word.strip():\n",
        "                    continue\n",
        "\n",
        "                # Лемматизация слова и определение его части речи\n",
        "                parsed_word = morph_analyzer.parse(word.lower())[0]\n",
        "                lemma = parsed_word.normal_form\n",
        "                pos = parsed_word.tag.POS\n",
        "\n",
        "                # Формируем строку из леммы и части речи\n",
        "                lemma_info = f\"{lemma} {pos}\"\n",
        "\n",
        "                # Добавляем лемму в множество уникальных лемм\n",
        "                unique_lemmas.add(lemma_info)\n",
        "\n",
        "            # Количество уникальных лемм в файле\n",
        "            unique_lemma_count = len(unique_lemmas)\n",
        "            unique_lemma_counts.append(unique_lemma_count)\n",
        "\n",
        "    # Вычисление статистик для текущего уровня CEFR\n",
        "    mean_unique_lemma_count = np.mean(unique_lemma_counts)\n",
        "    median_unique_lemma_count = np.median(unique_lemma_counts)\n",
        "    sd_unique_lemma_count = np.std(unique_lemma_counts)\n",
        "\n",
        "    # Сохранение статистик в словарь\n",
        "    unique_lemma_stats[cefr_level] = {\n",
        "        'mean': mean_unique_lemma_count,\n",
        "        'median': median_unique_lemma_count,\n",
        "        'sd': sd_unique_lemma_count\n",
        "    }\n",
        "\n",
        "# Вывод результатов\n",
        "print(\"Статистики для среднего количества уникальных лемм по уровням CEFR:\")\n",
        "for cefr_level in cefr_levels:\n",
        "    stats = unique_lemma_stats[cefr_level]\n",
        "    print(f\"{cefr_level}: Mean = {stats['mean']}, Median = {stats['median']}, SD = {stats['sd']}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JH02IDF6OL5d",
        "outputId": "dfdc68ac-ae9f-4d4a-d172-dbc93ce7821e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Статистики для среднего количества уникальных лемм по уровням CEFR:\n",
            "A1: Mean = 36.34920634920635, Median = 37.0, SD = 19.325251783947746\n",
            "A2: Mean = 75.61872146118722, Median = 73.0, SD = 36.39444380558453\n",
            "B1: Mean = 106.06609808102345, Median = 104.0, SD = 40.17926118987714\n",
            "B2: Mean = 139.84251968503938, Median = 143.0, SD = 37.72787461261296\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Среднее количество служебных слов"
      ],
      "metadata": {
        "id": "J0HXOWxPQIi0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import numpy as np\n",
        "\n",
        "# Устанавливаем стоп-слова для русского языка\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('russian'))\n",
        "\n",
        "# Функция для очистки текста от тегов и приведения к нижнему регистру\n",
        "def clean_text(text):\n",
        "    # Удаляем теги и токенизируем текст\n",
        "    cleaned_text = re.sub(r'<[^>]+>|', '', text)\n",
        "    words = word_tokenize(cleaned_text.lower(), language='russian')\n",
        "    return words\n",
        "\n",
        "# Базовая директория\n",
        "base_directory = \"/content/Students_texts\"\n",
        "cefr_levels = ['A1', 'A2', 'B1', 'B2']\n",
        "\n",
        "# Словарь для хранения статистик по уровням CEFR\n",
        "functional_words_stats = {}\n",
        "\n",
        "# Обработка каждого уровня CEFR\n",
        "for cefr_level in cefr_levels:\n",
        "    level_directory = os.path.join(base_directory, cefr_level)\n",
        "    functional_words_percentages = []\n",
        "\n",
        "    for filename in os.listdir(level_directory):\n",
        "        if filename.endswith(\".txt\"):\n",
        "            filepath = os.path.join(level_directory, filename)\n",
        "            with open(filepath, \"r\", encoding=\"utf-8\") as file:\n",
        "                text = file.read()\n",
        "                words = clean_text(text)\n",
        "\n",
        "                # Подсчёт количества служебных слов и общего количества слов\n",
        "                functional_words_count = sum(1 for word in words if word in stop_words)\n",
        "                total_words_count = len(words)\n",
        "\n",
        "                # Вычисляем процент служебных слов от общего количества слов\n",
        "                percent_functional_words = (functional_words_count / total_words_count) * 100\n",
        "                functional_words_percentages.append(percent_functional_words)\n",
        "\n",
        "    # Вычисляем статистики для текущего уровня CEFR\n",
        "    mean_functional_words = np.mean(functional_words_percentages)\n",
        "    median_functional_words = np.median(functional_words_percentages)\n",
        "    sd_functional_words = np.std(functional_words_percentages)\n",
        "\n",
        "    # Сохраняем статистики в словарь\n",
        "    functional_words_stats[cefr_level] = {\n",
        "        'mean': mean_functional_words,\n",
        "        'median': median_functional_words,\n",
        "        'sd': sd_functional_words\n",
        "    }\n",
        "\n",
        "# Выводим результаты\n",
        "print(\"Статистики для процента служебных слов от общего количества слов по уровням CEFR:\")\n",
        "for cefr_level in cefr_levels:\n",
        "    stats = functional_words_stats[cefr_level]\n",
        "    print(f\"{cefr_level}: Mean = {stats['mean']}%, Median = {stats['median']}%, SD = {stats['sd']}%\")"
      ],
      "metadata": {
        "id": "gpjB8Qj8T597",
        "outputId": "162347d1-f332-478e-f45d-02855a956e55",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Статистики для процента служебных слов от общего количества слов по уровням CEFR:\n",
            "A1: Mean = 31.771854299949585%, Median = 29.88505747126437%, SD = 10.18900687094465%\n",
            "A2: Mean = 33.75403279757885%, Median = 33.73863552368985%, SD = 6.246294365661413%\n",
            "B1: Mean = 32.69952889890615%, Median = 32.88135593220339%, SD = 4.8327602626019495%\n",
            "B2: Mean = 31.295963078536392%, Median = 30.909090909090907%, SD = 4.125081492279947%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "MTLD и HD-D по токенам"
      ],
      "metadata": {
        "id": "lNTfUWFBa_RY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install lexicalrichness"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X_wwO36mc-dN",
        "outputId": "b2c5a339-cb40-4958-cc20-40ec241a6d1a"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting lexicalrichness\n",
            "  Downloading lexicalrichness-0.5.1.tar.gz (97 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/97.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m97.8/97.8 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from lexicalrichness) (1.13.1)\n",
            "Requirement already satisfied: textblob>=0.15.3 in /usr/local/lib/python3.10/dist-packages (from lexicalrichness) (0.17.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from lexicalrichness) (2.1.4)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from lexicalrichness) (3.7.1)\n",
            "Requirement already satisfied: numpy<2.3,>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from scipy>=1.0.0->lexicalrichness) (1.26.4)\n",
            "Requirement already satisfied: nltk>=3.1 in /usr/local/lib/python3.10/dist-packages (from textblob>=0.15.3->lexicalrichness) (3.8.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->lexicalrichness) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->lexicalrichness) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->lexicalrichness) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->lexicalrichness) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->lexicalrichness) (24.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->lexicalrichness) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->lexicalrichness) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->lexicalrichness) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->lexicalrichness) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->lexicalrichness) (2024.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob>=0.15.3->lexicalrichness) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob>=0.15.3->lexicalrichness) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob>=0.15.3->lexicalrichness) (2024.5.15)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob>=0.15.3->lexicalrichness) (4.66.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->lexicalrichness) (1.16.0)\n",
            "Building wheels for collected packages: lexicalrichness\n",
            "  Building wheel for lexicalrichness (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for lexicalrichness: filename=lexicalrichness-0.5.1-py3-none-any.whl size=15416 sha256=e32a442642b94d82955e98f9496096a92c2d951d882f5189567d851213fe2444\n",
            "  Stored in directory: /root/.cache/pip/wheels/cd/ba/80/d4dabc1bf242a672ffc00226a2303a7471bb841c0872b2c212\n",
            "Successfully built lexicalrichness\n",
            "Installing collected packages: lexicalrichness\n",
            "Successfully installed lexicalrichness-0.5.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from lexicalrichness import LexicalRichness\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from collections import defaultdict\n",
        "\n",
        "# Загрузка необходимых ресурсов NLTK\n",
        "nltk.download('punkt')\n",
        "\n",
        "class MetricsCalculator:\n",
        "    def __init__(self, threshold=0.72):\n",
        "        self.threshold = threshold\n",
        "        self.hd_data = defaultdict(list)\n",
        "        self.mtld_data = defaultdict(list)\n",
        "        self.file_names = []\n",
        "\n",
        "    def preprocess_text(self, text):\n",
        "        # Удаление тегов и перевод текста в нижний регистр\n",
        "        text = re.sub(r'<.*?>', '', text)\n",
        "        text = re.sub(r'\\n', '', text)\n",
        "        return text\n",
        "\n",
        "    def process_files(self, directory):\n",
        "        cefr_levels = ['A1', 'A2', 'B1', 'B2']\n",
        "\n",
        "        for cefr_level in cefr_levels:\n",
        "            cefr_dir = os.path.join(directory, cefr_level)\n",
        "            if os.path.isdir(cefr_dir):\n",
        "                for filename in os.listdir(cefr_dir):\n",
        "                    if filename.endswith(\".txt\"):\n",
        "                        filepath = os.path.join(cefr_dir, filename)\n",
        "                        with open(filepath, \"r\", encoding=\"utf-8\") as file:\n",
        "                            text = file.read()\n",
        "                            preprocessed_text = self.preprocess_text(text)\n",
        "                            tokens = word_tokenize(preprocessed_text)\n",
        "                            if len(tokens) >= 50:  # Фильтрация текстов с менее чем 50 токенами\n",
        "                                lex = LexicalRichness(preprocessed_text)\n",
        "                                hd = lex.hdd(draws=20)\n",
        "                                mtld = lex.mtld(threshold=self.threshold)\n",
        "                                self.hd_data[cefr_level].append(hd)\n",
        "                                self.mtld_data[cefr_level].append(mtld)\n",
        "                                self.file_names.append(filename)\n",
        "\n",
        "    def calculate_group_stats(self, metric_data):\n",
        "        stats = {}\n",
        "        for cefr_level, values in metric_data.items():\n",
        "            mean_value = np.mean(values)\n",
        "            median_value = np.median(values)\n",
        "            sd_value = np.std(values)\n",
        "            stats[cefr_level] = {\n",
        "                \"mean\": mean_value,\n",
        "                \"median\": median_value,\n",
        "                \"sd\": sd_value\n",
        "            }\n",
        "        return stats\n",
        "\n",
        "    def print_group_stats(self, stats, metric_name):\n",
        "        print(f\"\\n--- Статистики для {metric_name} ---\")\n",
        "        for cefr_level, metrics in stats.items():\n",
        "            print(f\"CEFR Level: {cefr_level}\")\n",
        "            print(f\"  Mean: {metrics['mean']:.4f}\")\n",
        "            print(f\"  Median: {metrics['median']:.4f}\")\n",
        "            print(f\"  SD: {metrics['sd']:.4f}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    directory_path = \"/content/Students_texts\"\n",
        "\n",
        "    metrics_calculator = MetricsCalculator(threshold=0.72)\n",
        "    metrics_calculator.process_files(directory_path)\n",
        "\n",
        "    # Calculate statistics for HD-D and MTLD for each CEFR level\n",
        "    hd_stats = metrics_calculator.calculate_group_stats(metrics_calculator.hd_data)\n",
        "    mtld_stats = metrics_calculator.calculate_group_stats(metrics_calculator.mtld_data)\n",
        "\n",
        "    # Print statistics for HD-D and MTLD\n",
        "    metrics_calculator.print_group_stats(hd_stats, 'HD-D')\n",
        "    metrics_calculator.print_group_stats(mtld_stats, 'MTLD')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1XDiVujybEhu",
        "outputId": "95e6b131-9acf-48b4-dffc-06bf5d49082d"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Статистики для HD-D ---\n",
            "CEFR Level: A1\n",
            "  Mean: 0.8680\n",
            "  Median: 0.8702\n",
            "  SD: 0.0428\n",
            "CEFR Level: A2\n",
            "  Mean: 0.9041\n",
            "  Median: 0.9081\n",
            "  SD: 0.0296\n",
            "CEFR Level: B1\n",
            "  Mean: 0.9248\n",
            "  Median: 0.9276\n",
            "  SD: 0.0232\n",
            "CEFR Level: B2\n",
            "  Mean: 0.9365\n",
            "  Median: 0.9393\n",
            "  SD: 0.0165\n",
            "\n",
            "--- Статистики для MTLD ---\n",
            "CEFR Level: A1\n",
            "  Mean: 55.5288\n",
            "  Median: 46.3905\n",
            "  SD: 28.3161\n",
            "CEFR Level: A2\n",
            "  Mean: 84.2128\n",
            "  Median: 78.5000\n",
            "  SD: 34.9302\n",
            "CEFR Level: B1\n",
            "  Mean: 121.0944\n",
            "  Median: 113.3572\n",
            "  SD: 47.3512\n",
            "CEFR Level: B2\n",
            "  Mean: 157.0457\n",
            "  Median: 151.2127\n",
            "  SD: 53.3487\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "MTLD и HD-D по леммам"
      ],
      "metadata": {
        "id": "It9sHFhrg4XS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from collections import defaultdict\n",
        "from lexicalrichness import LexicalRichness\n",
        "import pymorphy2\n",
        "\n",
        "# Загрузка необходимых ресурсов NLTK\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('russian'))\n",
        "\n",
        "class MetricsCalculator:\n",
        "    morph_analyzer = pymorphy2.MorphAnalyzer()\n",
        "\n",
        "    def __init__(self, threshold=0.72):\n",
        "        self.threshold = threshold\n",
        "        self.hd_data = defaultdict(list)\n",
        "        self.mtld_data = defaultdict(list)\n",
        "        self.file_names = []\n",
        "\n",
        "    def preprocess_text(self, text):\n",
        "        # Удаление тегов и перевод текста в нижний регистр\n",
        "        text = text.lower()\n",
        "        text = re.sub(r'<.*?>', '', text)\n",
        "        text = re.sub(r'\\n', '', text)\n",
        "        return text\n",
        "\n",
        "    def lemmatize_text(self, text):\n",
        "        lemmas = [self.morph_analyzer.parse(token)[0].normal_form for token in word_tokenize(text)]\n",
        "        return lemmas\n",
        "\n",
        "    def process_files(self, directory):\n",
        "        cefr_levels = ['A1', 'A2', 'B1', 'B2']\n",
        "\n",
        "        for cefr_level in cefr_levels:\n",
        "            cefr_dir = os.path.join(directory, cefr_level)\n",
        "            if os.path.isdir(cefr_dir):\n",
        "                for filename in os.listdir(cefr_dir):\n",
        "                    if filename.endswith(\".txt\"):\n",
        "                        filepath = os.path.join(cefr_dir, filename)\n",
        "                        with open(filepath, \"r\", encoding=\"utf-8\") as file:\n",
        "                            text = file.read()\n",
        "                            preprocessed_text = self.preprocess_text(text)\n",
        "                            lemmas = self.lemmatize_text(preprocessed_text)\n",
        "                            if len(lemmas) >= 50:  # Фильтрация текстов с менее чем 50 леммами\n",
        "                                lemmas_str = ' '.join(lemmas)\n",
        "                                lex = LexicalRichness(lemmas_str)\n",
        "                                hd = lex.hdd(draws=20)\n",
        "                                mtld = lex.mtld(threshold=self.threshold)\n",
        "                                self.hd_data[cefr_level].append(hd)\n",
        "                                self.mtld_data[cefr_level].append(mtld)\n",
        "                                self.file_names.append(filename)\n",
        "\n",
        "    def calculate_group_stats(self, metric_data):\n",
        "        stats = {}\n",
        "        for cefr_level, values in metric_data.items():\n",
        "            mean_value = np.mean(values)\n",
        "            median_value = np.median(values)\n",
        "            sd_value = np.std(values)\n",
        "            stats[cefr_level] = {\n",
        "                \"mean\": mean_value,\n",
        "                \"median\": median_value,\n",
        "                \"sd\": sd_value\n",
        "            }\n",
        "        return stats\n",
        "\n",
        "    def print_group_stats(self, stats, metric_name):\n",
        "        print(f\"\\n--- Статистики для {metric_name} ---\")\n",
        "        for cefr_level, metrics in stats.items():\n",
        "            print(f\"CEFR Level: {cefr_level}\")\n",
        "            print(f\"  Mean: {metrics['mean']:.4f}\")\n",
        "            print(f\"  Median: {metrics['median']:.4f}\")\n",
        "            print(f\"  SD: {metrics['sd']:.4f}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    directory_path = \"/content/Students_texts\"\n",
        "\n",
        "    metrics_calculator = MetricsCalculator(threshold=0.72)\n",
        "    metrics_calculator.process_files(directory_path)\n",
        "\n",
        "    # Calculate statistics for HD-D and MTLD for each CEFR level\n",
        "    hd_stats = metrics_calculator.calculate_group_stats(metrics_calculator.hd_data)\n",
        "    mtld_stats = metrics_calculator.calculate_group_stats(metrics_calculator.mtld_data)\n",
        "\n",
        "    # Print statistics for HD-D and MTLD\n",
        "    metrics_calculator.print_group_stats(hd_stats, 'HD-D')\n",
        "    metrics_calculator.print_group_stats(mtld_stats, 'MTLD')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L6TMI5Vag4xC",
        "outputId": "a6626fd1-0a1f-4e8d-823f-71bce43a4199"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Статистики для HD-D ---\n",
            "CEFR Level: A1\n",
            "  Mean: 0.8402\n",
            "  Median: 0.8411\n",
            "  SD: 0.0441\n",
            "CEFR Level: A2\n",
            "  Mean: 0.8720\n",
            "  Median: 0.8753\n",
            "  SD: 0.0334\n",
            "CEFR Level: B1\n",
            "  Mean: 0.8960\n",
            "  Median: 0.8985\n",
            "  SD: 0.0279\n",
            "CEFR Level: B2\n",
            "  Mean: 0.9129\n",
            "  Median: 0.9150\n",
            "  SD: 0.0196\n",
            "\n",
            "--- Статистики для MTLD ---\n",
            "CEFR Level: A1\n",
            "  Mean: 41.9862\n",
            "  Median: 35.5012\n",
            "  SD: 19.3967\n",
            "CEFR Level: A2\n",
            "  Mean: 53.9181\n",
            "  Median: 51.4733\n",
            "  SD: 18.5486\n",
            "CEFR Level: B1\n",
            "  Mean: 71.5228\n",
            "  Median: 66.7383\n",
            "  SD: 26.6681\n",
            "CEFR Level: B2\n",
            "  Mean: 85.5712\n",
            "  Median: 83.5513\n",
            "  SD: 25.5841\n"
          ]
        }
      ]
    }
  ]
}